{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spaCy-Learn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPA9lCtTIs54SOFEpYQqd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XinyueChen-Flora/NLP-Learn/blob/main/spaCy_Learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2t2FbL2rIho4"
      },
      "outputs": [],
      "source": [
        "# import spacy and create a blanc english nlp object\n",
        "import spacy\n",
        "nlp = spacy.blank(\"en\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Created by processing a string of text with the nlp object\n",
        "doc = nlp(\"I like tree kangaroos and narwhals.\")"
      ],
      "metadata": {
        "id": "FDd1R0HuImIN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over tokens in a Doc\n",
        "for token in doc:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhJtUJiXItWx",
        "outputId": "25fa20f1-59c8-4393-ff0c-06d481315525"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "like\n",
            "kangaroos\n",
            "and\n",
            "narwhals\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Index into the Doc to get a single Token\n",
        "first_token = doc[0]\n",
        "# Get the token via the .text attribute\n",
        "print(first_token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZLrfUoaJOvV",
        "outputId": "c28e0edc-e642-4f14-8b6c-85c6424d22f6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A slice of the Doc for \"tree kangaroos\"\n",
        "tree_kangaroos = doc[2:4]\n",
        "print(tree_kangaroos.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thap2Tw6Jsd5",
        "outputId": "d1ac87b5-359d-4e8e-f9c7-d83e5a99f95d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tree kangaroos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i is the index of the token within the parent document.\n",
        "# text returns the token text.\n",
        "# is_alpha, is_punct and like_num return boolean values indicating whether the token consists of alphabetic characters, whether it's punctuation or whether it resembles a number. \n",
        "# For example, a token \"10\" – one, zero – or the word \"ten\" – T, E, N.\n",
        "# These attributes are also called lexical attributes: they refer to the entry in the vocabulary and don't depend on the token's context.\n",
        "print(\"Index:   \", [token.i for token in doc])\n",
        "print(\"Text:    \", [token.text for token in doc])\n",
        "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
        "print(\"is_punct:\", [token.is_punct for token in doc])\n",
        "print(\"like_num:\", [token.like_num for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1JghdyaJ1yq",
        "outputId": "8514490c-c350-4cbe-ee4e-2f4b9c939022"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index:    [0, 1, 2, 3, 4, 5]\n",
            "Text:     ['I', 'like', 'kangaroos', 'and', 'narwhals', '.']\n",
            "is_alpha: [True, True, True, True, True, False]\n",
            "is_punct: [False, False, False, False, False, True]\n",
            "like_num: [False, False, False, False, False, False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
        "tree_kangaroos_and_narwhals = doc[2:6]\n",
        "print(tree_kangaroos_and_narwhals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-leKspDJLIbU",
        "outputId": "c113c240-6fd9-4285-a1e4-ceab33f33ef6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tree kangaroos and narwhals\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline"
      ],
      "metadata": {
        "id": "CYyru4pFNy91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting Part-of-speech tags"
      ],
      "metadata": {
        "id": "il0r6D-sODar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"She ate the pizza\")\n",
        "for token in doc:\n",
        "  # For each token in the doc, we can print the text and the .pos_ attribute, the predicted part-of-speech tag.\n",
        "  print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd9Dt2-HLgdd",
        "outputId": "b835e0c9-dc70-4614-c629-cf3c16319fc5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She PRON\n",
            "ate VERB\n",
            "the DET\n",
            "pizza NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting Syntactic Dependencies"
      ],
      "metadata": {
        "id": "kMHIP5rkOnyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token.text, token.pos_, token.dep_, token.head.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTrheaZfNLqH",
        "outputId": "71e55dad-b9ab-4e1b-953d-9106a3bc849f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She PRON nsubj ate\n",
            "ate VERB ROOT ate\n",
            "the DET det pizza\n",
            "pizza NOUN dobj ate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting Named Entities"
      ],
      "metadata": {
        "id": "HITPnAR2Pgk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBet20wUO1SW",
        "outputId": "f6cf2b46-ceaa-48cb-abb1-d7c94540fa73"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "U.K. GPE\n",
            "$1 billion MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain(\"GPE\")\n",
        "spacy.explain(\"det\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tXRFfseNPtif",
        "outputId": "0b7bd366-b6f1-4daf-c175-b73eb3e06087"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'determiner'"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rule-based matching"
      ],
      "metadata": {
        "id": "aW0FcuzHRxvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Matcher"
      ],
      "metadata": {
        "id": "tJ7H33yAS_V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "# Initialize the matcher with the shared vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "# Add the pattern to the matcher\n",
        "# The matcher.add method lets you add a pattern. \n",
        "# The first argument is a unique ID to identify which pattern was matched. \n",
        "# The second argument is a list of patterns.\n",
        "pattern = [{\"TEXT\":\"iPhone\"}, {\"TEXT\": \"X\"}]\n",
        "matcher.add(\"IPHONE_PATTERN\", [pattern])\n",
        "# Process some text\n",
        "doc = nlp(\"Upcoming iPhone X release data leaked\")\n",
        "# Call the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "# When you call the matcher on a doc, it returns a list of tuples.\n",
        "# Each tuple consists of three values: the match ID, the start index and the end index of the matched span.\n",
        "print(matches)\n",
        "# we can iterate over the matches and create a Span object: a slice of the doc at the start and end index.\n",
        "# match_id: hash value of the pattern name\n",
        "# start: start index of matched span\n",
        "# end: end index of matched span\n",
        "for match_id, start, end in matches:\n",
        "  matched_span = doc[start:end]\n",
        "  print(matched_span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TezBmIp-QCaX",
        "outputId": "4f0943e2-5628-4baf-c7c2-e9bd1fcd27d9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(9528407286733565721, 1, 3)]\n",
            "iPhone X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matching lexical attributes\n"
      ],
      "metadata": {
        "id": "RImpbBz7UU8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = [\n",
        "    {\"IS_DIGIT\": True},\n",
        "    {\"LOWER\": \"fifa\"},\n",
        "    {\"LOWER\": \"world\"},\n",
        "    {\"LOWER\": \"cup\"},\n",
        "    {\"IS_PUNCT\": True}\n",
        "]\n",
        "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
        "matcher.add(\"FIFA_PATTERN\", [pattern])\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "  matched_span = doc[start:end]\n",
        "  print(matched_span.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTW6PT4KTYH7",
        "outputId": "9acd3831-811f-4ba1-93c6-f29b7951a64d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018 FIFA World Cup:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matching other token attributes"
      ],
      "metadata": {
        "id": "kQqaMVIXVDUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = [\n",
        "           {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
        "           {\"POS\": \"NOUN\"}\n",
        "]\n",
        "doc = nlp(\"I loved dogs but now I love cats more\")\n",
        "matcher.add(\"LOVE_PATTERN\", [pattern])\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "  matched_span = doc[start:end]\n",
        "  print(matched_span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFIR4UeoUdjG",
        "outputId": "9c150484-f5ad-412d-f855-93dbfcc55e4b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loved dogs\n",
            "love cats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using operators and quantifiers"
      ],
      "metadata": {
        "id": "rDTRhYlAVse2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = [\n",
        "           {\"LEMMA\": \"buy\"},\n",
        "           {\"POS\": \"DET\",\"OP\":\"?\"}, # optional: match 0 or 1 times\n",
        "           {\"POS\": \"NOUN\"}\n",
        "]\n",
        "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
        "matcher.add(\"buy_PATTERN\", [pattern])\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "  matched_span = doc[start:end]\n",
        "  print(matched_span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycjj0XPlVurT",
        "outputId": "dbc20688-0b30-4534-838f-e72e7c05cf66"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bought a smartphone\n",
            "buying apps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large-scale data analysis with spaCy"
      ],
      "metadata": {
        "id": "GP3O2kOucp_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Structures"
      ],
      "metadata": {
        "id": "HivJfP-Fcp3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Strings to hashes\n",
        "doc = nlp(\"I have a cat\")\n",
        "# Look up the hash for the work \"cat\"\n",
        "cat_hash = nlp.vocab.strings[\"cat\"]\n",
        "print(cat_hash)\n",
        "\n",
        "# Look up the cat_hash to get the string\n",
        "cat_string = nlp.vocab.strings[cat_hash]\n",
        "print(cat_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-yOZjN1WIq-",
        "outputId": "e4b89596-58bb-47db-db0f-eecca79eaba2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5439657043933447811\n",
            "cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Doc, Span and Token\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "# The words and spaces to create the doc from\n",
        "words = [\"Hello\", \"world\", \"!\"]\n",
        "spaces = [True, False, False]\n",
        "\n",
        "# Create a doc manually\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc)\n",
        "\n",
        "# Create a span manually\n",
        "span = Span(doc, 0, 2)\n",
        "\n",
        "# Create a span with a label\n",
        "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
        "\n",
        "# Add span to the doc.ents\n",
        "doc.ents = [span_with_label]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IxqhhpHbm_n",
        "outputId": "f1efc469-c60d-4c04-b790-34b89e1380bf"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world!\n",
            "(Hello world,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word vectors and semantic similarity"
      ],
      "metadata": {
        "id": "eOWENmmhlUD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# solve the problem of cannot find the model\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_md\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJnNIGeUzBO7",
        "outputId": "b42150fd-dcfd-4f23-9f42-197f68f28b13"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()\n",
        "\n",
        "# Compare two documents\n",
        "doc1 = nlp(\"I like fast food\")\n",
        "doc2 = nlp(\"I like pizza\")\n",
        "print(doc1.similarity(doc2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP8enY_BgExz",
        "outputId": "146553a3-c713-4d84-f6ce-6ed229be35b4"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8627204117787385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare a document with a token\n",
        "doc = nlp(\"I like pizza\")\n",
        "token = nlp(\"soap\")[0]\n",
        "\n",
        "print(doc.similarity(token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSnUTVQHlj-h",
        "outputId": "7091ca87-03b4-4c08-d9ea-7e98341b7603"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.32531983166759537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare a span with a document\n",
        "span = nlp(\"I like pizza and pasta\")[2:5]\n",
        "doc = nlp(\"McDonalds sells burgers\")\n",
        "print(span.similarity(doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t9-ieJlzwB8",
        "outputId": "b53a07f1-074b-4b63-e31b-97b8d04d1e41"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6199092090831612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word vectors in spaCy\n",
        "doc = nlp(\"I have a bananna\")\n",
        "# Access the vector via the token.vector attribute\n",
        "print(doc[3].vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxBP5qEjz63d",
        "outputId": "af9a20e8-bdf5-4eb3-906e-ff5005f36635"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.0228e-01 -7.6618e-02  3.7032e-01  3.2845e-02 -4.1957e-01  7.2069e-02\n",
            " -3.7476e-01  5.7460e-02 -1.2401e-02  5.2949e-01 -5.2380e-01 -1.9771e-01\n",
            " -3.4147e-01  5.3317e-01 -2.5331e-02  1.7380e-01  1.6772e-01  8.3984e-01\n",
            "  5.5107e-02  1.0547e-01  3.7872e-01  2.4275e-01  1.4745e-02  5.5951e-01\n",
            "  1.2521e-01 -6.7596e-01  3.5842e-01 -4.0028e-02  9.5949e-02 -5.0690e-01\n",
            " -8.5318e-02  1.7980e-01  3.3867e-01  1.3230e-01  3.1021e-01  2.1878e-01\n",
            "  1.6853e-01  1.9874e-01 -5.7385e-01 -1.0649e-01  2.6669e-01  1.2838e-01\n",
            " -1.2803e-01 -1.3284e-01  1.2657e-01  8.6723e-01  9.6721e-02  4.8306e-01\n",
            "  2.1271e-01 -5.4990e-02 -8.2425e-02  2.2408e-01  2.3975e-01 -6.2260e-02\n",
            "  6.2194e-01 -5.9900e-01  4.3201e-01  2.8143e-01  3.3842e-02 -4.8815e-01\n",
            " -2.1359e-01  2.7401e-01  2.4095e-01  4.5950e-01 -1.8605e-01 -1.0497e+00\n",
            " -9.7305e-02 -1.8908e-01 -7.0929e-01  4.0195e-01 -1.8768e-01  5.1687e-01\n",
            "  1.2520e-01  8.4150e-01  1.2097e-01  8.8239e-02 -2.9196e-02  1.2151e-03\n",
            "  5.6825e-02 -2.7421e-01  2.5564e-01  6.9793e-02 -2.2258e-01 -3.6006e-01\n",
            " -2.2402e-01 -5.3699e-02  1.2022e+00  5.4535e-01 -5.7998e-01  1.0905e-01\n",
            "  4.2167e-01  2.0662e-01  1.2936e-01 -4.1457e-02 -6.6777e-01  4.0467e-01\n",
            " -1.5218e-02 -2.7640e-01 -1.5611e-01 -7.9198e-02  4.0037e-02 -1.2944e-01\n",
            " -2.4090e-04 -2.6785e-01 -3.8115e-01 -9.7245e-01  3.1726e-01 -4.3951e-01\n",
            "  4.1934e-01  1.8353e-01 -1.5260e-01 -1.0808e-01 -1.0358e+00  7.6217e-02\n",
            "  1.6519e-01  2.6526e-04  1.6616e-01 -1.5281e-01  1.8123e-01  7.0274e-01\n",
            "  5.7956e-03  5.1664e-02 -5.9745e-02 -2.7551e-01 -3.9049e-01  6.1132e-02\n",
            "  5.5430e-01 -8.7997e-02 -4.1681e-01  3.2826e-01 -5.2549e-01 -4.4288e-01\n",
            "  8.2183e-03  2.4486e-01 -2.2982e-01 -3.4981e-01  2.6894e-01  3.9166e-01\n",
            " -4.1904e-01  1.6191e-01 -2.6263e+00  6.4134e-01  3.9743e-01 -1.2868e-01\n",
            " -3.1946e-01 -2.5633e-01 -1.2220e-01  3.2275e-01 -7.9933e-02 -1.5348e-01\n",
            "  3.1505e-01  3.0591e-01  2.6012e-01  1.8553e-01 -2.4043e-01  4.2886e-02\n",
            "  4.0622e-01 -2.4256e-01  6.3870e-01  6.9983e-01 -1.4043e-01  2.5209e-01\n",
            "  4.8984e-01 -6.1067e-02 -3.6766e-01 -5.5089e-01 -3.8265e-01 -2.0843e-01\n",
            "  2.2832e-01  5.1218e-01  2.7868e-01  4.7652e-01  4.7951e-02 -3.4008e-01\n",
            " -3.2873e-01 -4.1967e-01 -7.5499e-02 -3.8954e-01 -2.9622e-02 -3.4070e-01\n",
            "  2.2170e-01 -6.2856e-02 -5.1903e-01 -3.7774e-01 -4.3477e-03 -5.8301e-01\n",
            " -8.7546e-02 -2.3929e-01 -2.4711e-01 -2.5887e-01 -2.9894e-01  1.3715e-01\n",
            "  2.9892e-02  3.6544e-02 -4.9665e-01 -1.8160e-01  5.2939e-01  2.1992e-01\n",
            " -4.4514e-01  3.7798e-01 -5.7062e-01 -4.6946e-02  8.1806e-02  1.9279e-02\n",
            "  3.3246e-01 -1.4620e-01  1.7156e-01  3.9981e-01  3.6217e-01  1.2816e-01\n",
            "  3.1644e-01  3.7569e-01 -7.4690e-02 -4.8480e-02 -3.1401e-01 -1.9286e-01\n",
            " -3.1294e-01 -1.7553e-02 -1.7514e-01 -2.7587e-02 -1.0000e+00  1.8387e-01\n",
            "  8.1434e-01 -1.8913e-01  5.0999e-01 -9.1960e-03 -1.9295e-03  2.8189e-01\n",
            "  2.7247e-02  4.3409e-01 -5.4967e-01 -9.7426e-02 -2.4540e-01 -1.7203e-01\n",
            " -8.8650e-02 -3.0298e-01 -1.3591e-01 -2.7765e-01  3.1286e-03  2.0556e-01\n",
            " -1.5772e-01 -5.2308e-01 -6.4701e-01 -3.7014e-01  6.9393e-02  1.1401e-01\n",
            "  2.7594e-01 -1.3875e-01 -2.7268e-01  6.6891e-01 -5.6454e-02  2.4017e-01\n",
            " -2.6730e-01  2.9860e-01  1.0083e-01  5.5592e-01  3.2849e-01  7.6858e-02\n",
            "  1.5528e-01  2.5636e-01 -1.0772e-01 -1.2359e-01  1.1827e-01 -9.9029e-02\n",
            " -3.4328e-01  1.1502e-01 -3.7808e-01 -3.9012e-02 -3.4593e-01 -1.9404e-01\n",
            " -3.3580e-01 -6.2334e-02  2.8919e-01  2.8032e-01 -5.3741e-01  6.2794e-01\n",
            "  5.6955e-02  6.2147e-01 -2.5282e-01  4.1670e-01 -1.0108e-02 -2.5434e-01\n",
            "  4.0003e-01  4.2432e-01  2.2672e-01  1.7553e-01  2.3049e-01  2.8323e-01\n",
            "  1.3882e-01  3.1218e-03  1.7057e-01  3.6685e-01  2.5247e-03 -6.4009e-01\n",
            " -2.9765e-01  7.8943e-01  3.3168e-01 -1.1966e+00 -4.7156e-02  5.3175e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(\"I like cats\")\n",
        "doc2 = nlp(\"I hate cats\")\n",
        "print(doc1.similarity(doc2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7JDkHli1ynj",
        "outputId": "8c0bc4aa-40aa-4ab8-ad3b-aab5f0eea4a0"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9501447503553421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining predictions and rules"
      ],
      "metadata": {
        "id": "P0NHCHxt2c71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adding statistical predictions\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"DOG\", [[{\"LOWER\": \"golden\"},{\"LOWER\": \"retriever\"}]])\n",
        "doc = nlp(\"I have a Golden Retriever\")\n",
        "\n",
        "for match_id, start, end in matcher(doc):\n",
        "  span = doc[start:end]\n",
        "  print(\"Matched span:\", span.text)\n",
        "  print(\"Root token:\", span.root.text)\n",
        "  print(\"Root head token:\", span.root.head.text)\n",
        "  print(\"Previous token:\", doc[start - 1].text, doc[start-1].pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5Zw334716pu",
        "outputId": "a5070153-e7ed-43d2-c049-22f7b9b83469"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched span: Golden Retriever\n",
            "Root token: Retriever\n",
            "Root head token: have\n",
            "Previous token: a DET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# efficient phrase matching\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "pattern = nlp(\"Golden Retriever\")\n",
        "matcher.add(\"DOG\", [pattern])\n",
        "doc = nlp(\"I have a Golden Retriever\")\n",
        "\n",
        "for match_id, start, end in matcher(doc):\n",
        "  span = doc[start:end]\n",
        "  print(\"Matched span:\", span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4Lldv7l3x8f",
        "outputId": "2f350023-1626-42d5-b5fd-160161df1fa0"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched span: Golden Retriever\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing Pipelines"
      ],
      "metadata": {
        "id": "sWDhLPMP7IiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NPeTfg-A4zPl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}