{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spaCy-Learn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMf52B3i3vld2CRmpm4fO4b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XinyueChen-Flora/NLP-Learn/blob/main/spaCy_Learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2t2FbL2rIho4"
      },
      "outputs": [],
      "source": [
        "# import spacy and create a blanc english nlp object\n",
        "import spacy\n",
        "nlp = spacy.blank(\"en\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Created by processing a string of text with the nlp object\n",
        "doc = nlp(\"I like tree kangaroos and narwhals.\")"
      ],
      "metadata": {
        "id": "FDd1R0HuImIN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over tokens in a Doc\n",
        "for token in doc:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhJtUJiXItWx",
        "outputId": "25fa20f1-59c8-4393-ff0c-06d481315525"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "like\n",
            "kangaroos\n",
            "and\n",
            "narwhals\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Index into the Doc to get a single Token\n",
        "first_token = doc[0]\n",
        "# Get the token via the .text attribute\n",
        "print(first_token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZLrfUoaJOvV",
        "outputId": "c28e0edc-e642-4f14-8b6c-85c6424d22f6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A slice of the Doc for \"tree kangaroos\"\n",
        "tree_kangaroos = doc[2:4]\n",
        "print(tree_kangaroos.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thap2Tw6Jsd5",
        "outputId": "d1ac87b5-359d-4e8e-f9c7-d83e5a99f95d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tree kangaroos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i is the index of the token within the parent document.\n",
        "# text returns the token text.\n",
        "# is_alpha, is_punct and like_num return boolean values indicating whether the token consists of alphabetic characters, whether it's punctuation or whether it resembles a number. \n",
        "# For example, a token \"10\" – one, zero – or the word \"ten\" – T, E, N.\n",
        "# These attributes are also called lexical attributes: they refer to the entry in the vocabulary and don't depend on the token's context.\n",
        "print(\"Index:   \", [token.i for token in doc])\n",
        "print(\"Text:    \", [token.text for token in doc])\n",
        "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
        "print(\"is_punct:\", [token.is_punct for token in doc])\n",
        "print(\"like_num:\", [token.like_num for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1JghdyaJ1yq",
        "outputId": "8514490c-c350-4cbe-ee4e-2f4b9c939022"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index:    [0, 1, 2, 3, 4, 5]\n",
            "Text:     ['I', 'like', 'kangaroos', 'and', 'narwhals', '.']\n",
            "is_alpha: [True, True, True, True, True, False]\n",
            "is_punct: [False, False, False, False, False, True]\n",
            "like_num: [False, False, False, False, False, False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
        "tree_kangaroos_and_narwhals = doc[2:6]\n",
        "print(tree_kangaroos_and_narwhals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-leKspDJLIbU",
        "outputId": "c113c240-6fd9-4285-a1e4-ceab33f33ef6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tree kangaroos and narwhals\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline"
      ],
      "metadata": {
        "id": "CYyru4pFNy91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting Part-of-speech tags"
      ],
      "metadata": {
        "id": "il0r6D-sODar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"She ate the pizza\")\n",
        "for token in doc:\n",
        "  # For each token in the doc, we can print the text and the .pos_ attribute, the predicted part-of-speech tag.\n",
        "  print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd9Dt2-HLgdd",
        "outputId": "b835e0c9-dc70-4614-c629-cf3c16319fc5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She PRON\n",
            "ate VERB\n",
            "the DET\n",
            "pizza NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting Syntactic Dependencies"
      ],
      "metadata": {
        "id": "kMHIP5rkOnyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token.text, token.pos_, token.dep_, token.head.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTrheaZfNLqH",
        "outputId": "71e55dad-b9ab-4e1b-953d-9106a3bc849f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She PRON nsubj ate\n",
            "ate VERB ROOT ate\n",
            "the DET det pizza\n",
            "pizza NOUN dobj ate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting Named Entities"
      ],
      "metadata": {
        "id": "HITPnAR2Pgk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBet20wUO1SW",
        "outputId": "f6cf2b46-ceaa-48cb-abb1-d7c94540fa73"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "U.K. GPE\n",
            "$1 billion MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain(\"GPE\")\n",
        "spacy.explain(\"det\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tXRFfseNPtif",
        "outputId": "0b7bd366-b6f1-4daf-c175-b73eb3e06087"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'determiner'"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rule-based matching"
      ],
      "metadata": {
        "id": "aW0FcuzHRxvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Matcher"
      ],
      "metadata": {
        "id": "tJ7H33yAS_V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "# Initialize the matcher with the shared vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "# Add the pattern to the matcher\n",
        "# The matcher.add method lets you add a pattern. \n",
        "# The first argument is a unique ID to identify which pattern was matched. \n",
        "# The second argument is a list of patterns.\n",
        "pattern = [{\"TEXT\":\"iPhone\"}, {\"TEXT\": \"X\"}]\n",
        "matcher.add(\"IPHONE_PATTERN\", [pattern])\n",
        "# Process some text\n",
        "doc = nlp(\"Upcoming iPhone X release data leaked\")\n",
        "# Call the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "# When you call the matcher on a doc, it returns a list of tuples.\n",
        "# Each tuple consists of three values: the match ID, the start index and the end index of the matched span.\n",
        "print(matches)\n",
        "# we can iterate over the matches and create a Span object: a slice of the doc at the start and end index.\n",
        "# match_id: hash value of the pattern name\n",
        "# start: start index of matched span\n",
        "# end: end index of matched span\n",
        "for match_id, start, end in matches:\n",
        "  matched_span = doc[start:end]\n",
        "  print(matched_span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TezBmIp-QCaX",
        "outputId": "4f0943e2-5628-4baf-c7c2-e9bd1fcd27d9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(9528407286733565721, 1, 3)]\n",
            "iPhone X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matching lexical attributes\n"
      ],
      "metadata": {
        "id": "RImpbBz7UU8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = [\n",
        "    {\"IS_DIGIT\": True},\n",
        "    {\"LOWER\": \"fifa\"},\n",
        "    {\"LOWER\": \"world\"},\n",
        "    {\"LOWER\": \"cup\"},\n",
        "    {\"IS_PUNCT\": True}\n",
        "]\n",
        "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
        "matcher.add(\"FIFA_PATTERN\", [pattern])\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "  matched_span = doc[start:end]\n",
        "  print(matched_span.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTW6PT4KTYH7",
        "outputId": "9acd3831-811f-4ba1-93c6-f29b7951a64d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018 FIFA World Cup:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matching other token attributes"
      ],
      "metadata": {
        "id": "kQqaMVIXVDUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = [\n",
        "           {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
        "           {\"POS\": \"NOUN\"}\n",
        "]\n",
        "doc = nlp(\"I loved dogs but now I love cats more\")\n",
        "matcher.add(\"LOVE_PATTERN\", [pattern])\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "  matched_span = doc[start:end]\n",
        "  print(matched_span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFIR4UeoUdjG",
        "outputId": "9c150484-f5ad-412d-f855-93dbfcc55e4b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loved dogs\n",
            "love cats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using operators and quantifiers"
      ],
      "metadata": {
        "id": "rDTRhYlAVse2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = [\n",
        "           {\"LEMMA\": \"buy\"},\n",
        "           {\"POS\": \"DET\",\"OP\":\"?\"}, # optional: match 0 or 1 times\n",
        "           {\"POS\": \"NOUN\"}\n",
        "]\n",
        "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
        "matcher.add(\"buy_PATTERN\", [pattern])\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "  matched_span = doc[start:end]\n",
        "  print(matched_span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycjj0XPlVurT",
        "outputId": "dbc20688-0b30-4534-838f-e72e7c05cf66"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bought a smartphone\n",
            "buying apps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large-scale data analysis with spaCy"
      ],
      "metadata": {
        "id": "GP3O2kOucp_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Structures"
      ],
      "metadata": {
        "id": "HivJfP-Fcp3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Strings to hashes\n",
        "doc = nlp(\"I have a cat\")\n",
        "# Look up the hash for the work \"cat\"\n",
        "cat_hash = nlp.vocab.strings[\"cat\"]\n",
        "print(cat_hash)\n",
        "\n",
        "# Look up the cat_hash to get the string\n",
        "cat_string = nlp.vocab.strings[cat_hash]\n",
        "print(cat_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-yOZjN1WIq-",
        "outputId": "e4b89596-58bb-47db-db0f-eecca79eaba2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5439657043933447811\n",
            "cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Doc, Span and Token\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "# The words and spaces to create the doc from\n",
        "words = [\"Hello\", \"world\", \"!\"]\n",
        "spaces = [True, False, False]\n",
        "\n",
        "# Create a doc manually\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc)\n",
        "\n",
        "# Create a span manually\n",
        "span = Span(doc, 0, 2)\n",
        "\n",
        "# Create a span with a label\n",
        "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
        "\n",
        "# Add span to the doc.ents\n",
        "doc.ents = [span_with_label]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IxqhhpHbm_n",
        "outputId": "f1efc469-c60d-4c04-b790-34b89e1380bf"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world!\n",
            "(Hello world,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word vectors and semantic similarity"
      ],
      "metadata": {
        "id": "eOWENmmhlUD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# solve the problem of cannot find the model\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_md\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJnNIGeUzBO7",
        "outputId": "b42150fd-dcfd-4f23-9f42-197f68f28b13"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()\n",
        "\n",
        "# Compare two documents\n",
        "doc1 = nlp(\"I like fast food\")\n",
        "doc2 = nlp(\"I like pizza\")\n",
        "print(doc1.similarity(doc2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP8enY_BgExz",
        "outputId": "146553a3-c713-4d84-f6ce-6ed229be35b4"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8627204117787385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare a document with a token\n",
        "doc = nlp(\"I like pizza\")\n",
        "token = nlp(\"soap\")[0]\n",
        "\n",
        "print(doc.similarity(token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSnUTVQHlj-h",
        "outputId": "7091ca87-03b4-4c08-d9ea-7e98341b7603"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.32531983166759537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare a span with a document\n",
        "span = nlp(\"I like pizza and pasta\")[2:5]\n",
        "doc = nlp(\"McDonalds sells burgers\")\n",
        "print(span.similarity(doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t9-ieJlzwB8",
        "outputId": "b53a07f1-074b-4b63-e31b-97b8d04d1e41"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6199092090831612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word vectors in spaCy\n",
        "doc = nlp(\"I have a bananna\")\n",
        "# Access the vector via the token.vector attribute\n",
        "print(doc[3].vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxBP5qEjz63d",
        "outputId": "af9a20e8-bdf5-4eb3-906e-ff5005f36635"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.0228e-01 -7.6618e-02  3.7032e-01  3.2845e-02 -4.1957e-01  7.2069e-02\n",
            " -3.7476e-01  5.7460e-02 -1.2401e-02  5.2949e-01 -5.2380e-01 -1.9771e-01\n",
            " -3.4147e-01  5.3317e-01 -2.5331e-02  1.7380e-01  1.6772e-01  8.3984e-01\n",
            "  5.5107e-02  1.0547e-01  3.7872e-01  2.4275e-01  1.4745e-02  5.5951e-01\n",
            "  1.2521e-01 -6.7596e-01  3.5842e-01 -4.0028e-02  9.5949e-02 -5.0690e-01\n",
            " -8.5318e-02  1.7980e-01  3.3867e-01  1.3230e-01  3.1021e-01  2.1878e-01\n",
            "  1.6853e-01  1.9874e-01 -5.7385e-01 -1.0649e-01  2.6669e-01  1.2838e-01\n",
            " -1.2803e-01 -1.3284e-01  1.2657e-01  8.6723e-01  9.6721e-02  4.8306e-01\n",
            "  2.1271e-01 -5.4990e-02 -8.2425e-02  2.2408e-01  2.3975e-01 -6.2260e-02\n",
            "  6.2194e-01 -5.9900e-01  4.3201e-01  2.8143e-01  3.3842e-02 -4.8815e-01\n",
            " -2.1359e-01  2.7401e-01  2.4095e-01  4.5950e-01 -1.8605e-01 -1.0497e+00\n",
            " -9.7305e-02 -1.8908e-01 -7.0929e-01  4.0195e-01 -1.8768e-01  5.1687e-01\n",
            "  1.2520e-01  8.4150e-01  1.2097e-01  8.8239e-02 -2.9196e-02  1.2151e-03\n",
            "  5.6825e-02 -2.7421e-01  2.5564e-01  6.9793e-02 -2.2258e-01 -3.6006e-01\n",
            " -2.2402e-01 -5.3699e-02  1.2022e+00  5.4535e-01 -5.7998e-01  1.0905e-01\n",
            "  4.2167e-01  2.0662e-01  1.2936e-01 -4.1457e-02 -6.6777e-01  4.0467e-01\n",
            " -1.5218e-02 -2.7640e-01 -1.5611e-01 -7.9198e-02  4.0037e-02 -1.2944e-01\n",
            " -2.4090e-04 -2.6785e-01 -3.8115e-01 -9.7245e-01  3.1726e-01 -4.3951e-01\n",
            "  4.1934e-01  1.8353e-01 -1.5260e-01 -1.0808e-01 -1.0358e+00  7.6217e-02\n",
            "  1.6519e-01  2.6526e-04  1.6616e-01 -1.5281e-01  1.8123e-01  7.0274e-01\n",
            "  5.7956e-03  5.1664e-02 -5.9745e-02 -2.7551e-01 -3.9049e-01  6.1132e-02\n",
            "  5.5430e-01 -8.7997e-02 -4.1681e-01  3.2826e-01 -5.2549e-01 -4.4288e-01\n",
            "  8.2183e-03  2.4486e-01 -2.2982e-01 -3.4981e-01  2.6894e-01  3.9166e-01\n",
            " -4.1904e-01  1.6191e-01 -2.6263e+00  6.4134e-01  3.9743e-01 -1.2868e-01\n",
            " -3.1946e-01 -2.5633e-01 -1.2220e-01  3.2275e-01 -7.9933e-02 -1.5348e-01\n",
            "  3.1505e-01  3.0591e-01  2.6012e-01  1.8553e-01 -2.4043e-01  4.2886e-02\n",
            "  4.0622e-01 -2.4256e-01  6.3870e-01  6.9983e-01 -1.4043e-01  2.5209e-01\n",
            "  4.8984e-01 -6.1067e-02 -3.6766e-01 -5.5089e-01 -3.8265e-01 -2.0843e-01\n",
            "  2.2832e-01  5.1218e-01  2.7868e-01  4.7652e-01  4.7951e-02 -3.4008e-01\n",
            " -3.2873e-01 -4.1967e-01 -7.5499e-02 -3.8954e-01 -2.9622e-02 -3.4070e-01\n",
            "  2.2170e-01 -6.2856e-02 -5.1903e-01 -3.7774e-01 -4.3477e-03 -5.8301e-01\n",
            " -8.7546e-02 -2.3929e-01 -2.4711e-01 -2.5887e-01 -2.9894e-01  1.3715e-01\n",
            "  2.9892e-02  3.6544e-02 -4.9665e-01 -1.8160e-01  5.2939e-01  2.1992e-01\n",
            " -4.4514e-01  3.7798e-01 -5.7062e-01 -4.6946e-02  8.1806e-02  1.9279e-02\n",
            "  3.3246e-01 -1.4620e-01  1.7156e-01  3.9981e-01  3.6217e-01  1.2816e-01\n",
            "  3.1644e-01  3.7569e-01 -7.4690e-02 -4.8480e-02 -3.1401e-01 -1.9286e-01\n",
            " -3.1294e-01 -1.7553e-02 -1.7514e-01 -2.7587e-02 -1.0000e+00  1.8387e-01\n",
            "  8.1434e-01 -1.8913e-01  5.0999e-01 -9.1960e-03 -1.9295e-03  2.8189e-01\n",
            "  2.7247e-02  4.3409e-01 -5.4967e-01 -9.7426e-02 -2.4540e-01 -1.7203e-01\n",
            " -8.8650e-02 -3.0298e-01 -1.3591e-01 -2.7765e-01  3.1286e-03  2.0556e-01\n",
            " -1.5772e-01 -5.2308e-01 -6.4701e-01 -3.7014e-01  6.9393e-02  1.1401e-01\n",
            "  2.7594e-01 -1.3875e-01 -2.7268e-01  6.6891e-01 -5.6454e-02  2.4017e-01\n",
            " -2.6730e-01  2.9860e-01  1.0083e-01  5.5592e-01  3.2849e-01  7.6858e-02\n",
            "  1.5528e-01  2.5636e-01 -1.0772e-01 -1.2359e-01  1.1827e-01 -9.9029e-02\n",
            " -3.4328e-01  1.1502e-01 -3.7808e-01 -3.9012e-02 -3.4593e-01 -1.9404e-01\n",
            " -3.3580e-01 -6.2334e-02  2.8919e-01  2.8032e-01 -5.3741e-01  6.2794e-01\n",
            "  5.6955e-02  6.2147e-01 -2.5282e-01  4.1670e-01 -1.0108e-02 -2.5434e-01\n",
            "  4.0003e-01  4.2432e-01  2.2672e-01  1.7553e-01  2.3049e-01  2.8323e-01\n",
            "  1.3882e-01  3.1218e-03  1.7057e-01  3.6685e-01  2.5247e-03 -6.4009e-01\n",
            " -2.9765e-01  7.8943e-01  3.3168e-01 -1.1966e+00 -4.7156e-02  5.3175e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(\"I like cats\")\n",
        "doc2 = nlp(\"I hate cats\")\n",
        "print(doc1.similarity(doc2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7JDkHli1ynj",
        "outputId": "8c0bc4aa-40aa-4ab8-ad3b-aab5f0eea4a0"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9501447503553421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining predictions and rules"
      ],
      "metadata": {
        "id": "P0NHCHxt2c71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adding statistical predictions\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"DOG\", [[{\"LOWER\": \"golden\"},{\"LOWER\": \"retriever\"}]])\n",
        "doc = nlp(\"I have a Golden Retriever\")\n",
        "\n",
        "for match_id, start, end in matcher(doc):\n",
        "  span = doc[start:end]\n",
        "  print(\"Matched span:\", span.text)\n",
        "  print(\"Root token:\", span.root.text)\n",
        "  print(\"Root head token:\", span.root.head.text)\n",
        "  print(\"Previous token:\", doc[start - 1].text, doc[start-1].pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5Zw334716pu",
        "outputId": "a5070153-e7ed-43d2-c049-22f7b9b83469"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched span: Golden Retriever\n",
            "Root token: Retriever\n",
            "Root head token: have\n",
            "Previous token: a DET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# efficient phrase matching\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "pattern = nlp(\"Golden Retriever\")\n",
        "matcher.add(\"DOG\", [pattern])\n",
        "doc = nlp(\"I have a Golden Retriever\")\n",
        "\n",
        "for match_id, start, end in matcher(doc):\n",
        "  span = doc[start:end]\n",
        "  print(\"Matched span:\", span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4Lldv7l3x8f",
        "outputId": "2f350023-1626-42d5-b5fd-160161df1fa0"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched span: Golden Retriever\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing Pipelines"
      ],
      "metadata": {
        "id": "sWDhLPMP7IiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pipline attributes\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "print(nlp.pipe_names)\n",
        "print(nlp.pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPeTfg-A4zPl",
        "outputId": "fa7e32c6-d2fe-4187-a8a4-94a642384e95"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tagger', 'parser', 'ner']\n",
            "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f8cfbed2d90>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7f8cfcf68a60>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7f8cfcca0130>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom pipeline components"
      ],
      "metadata": {
        "id": "nEP4j-5u9Ghc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "gnskEYst_7Ol",
        "outputId": "09a38c4b-0e29-430c-ed7e-fe886aa03c03"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy\n",
            "  Using cached spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.13)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: spacy\n",
            "Successfully installed spacy-3.2.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "spacy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_md\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxVce9dHF23q",
        "outputId": "21bed793-a32c-4a55-de98-3d91bd4a7970"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Anatomy of a component\n",
        "\n",
        "# Function that takes a doc, modifies it and returns it\n",
        "# Registered using the Language.component decorator\n",
        "# Can be added using the nlp.add_pipe method\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "# Create the nlp object\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Define a custom component\n",
        "@Language.component(\"custom_component\")\n",
        "def custom_component_function(doc):\n",
        "    # Print the doc's length\n",
        "    print(\"Doc length:\", len(doc))\n",
        "    # Return the doc object\n",
        "    return doc\n",
        "\n",
        "# Add the component first in the pipeline\n",
        "nlp.add_pipe(\"custom_component\", first=True)\n",
        "\n",
        "# Print the pipeline component names\n",
        "print(\"Pipeline:\", nlp.pipe_names)\n",
        "\n",
        "doc = nlp(\"Hello world!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfoLzGgq8Y_8",
        "outputId": "a7b3003a-8fc5-442f-beda-1980ea40691e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline: ['custom_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
            "Doc length: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In this exercise, you’ll be writing a custom component that uses the PhraseMatcher to find animal names in the document and adds the matched spans to the doc.ents. A PhraseMatcher with the animal patterns has already been created as the variable matcher.\n",
        "\n",
        "Define the custom component and apply the matcher to the doc.\n",
        "Create a Span for each match, assign the label ID for \"ANIMAL\" and overwrite the doc.ents with the new spans.\n",
        "Add the new component to the pipeline after the \"ner\" component.\n",
        "Process the text and print the entity text and entity label for the entities in doc.ents.\n",
        "\"\"\"\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.tokens import Span\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
        "animal_patterns = list(nlp.pipe(animals))\n",
        "print(\"animal_patterns:\", animal_patterns)\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "matcher.add(\"ANIMAL\", animal_patterns)\n",
        "\n",
        "# Define the custom component\n",
        "@Language.component(\"animal_component\")\n",
        "def animal_component_function(doc):\n",
        "    # Apply the matcher to the doc\n",
        "    matches = matcher(doc)\n",
        "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
        "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
        "    # Overwrite the doc.ents with the matched spans\n",
        "    doc.ents = spans\n",
        "    return doc\n",
        "\n",
        "\n",
        "# Add the component to the pipeline after the \"ner\" component\n",
        "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
        "print(nlp.pipe_names)\n",
        "\n",
        "# Process the text and print the text and label for the doc.ents\n",
        "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUR18fMGHmk",
        "outputId": "5bceb19a-35d9-4b18-b546-3f5d7f7bb361"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
            "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extension Attributes"
      ],
      "metadata": {
        "id": "0uNiDwRqHkDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attribute extensions\n",
        "from spacy.tokens import Token\n",
        "\n",
        "# Set extension on the Token with default value\n",
        "Token.set_extension(\"is_colors\", default=False)\n",
        "\n",
        "doc = nlp(\"The sky is blue.\")\n",
        "\n",
        "# Overwrite extension attribute value\n",
        "doc[3]._.is_color = True"
      ],
      "metadata": {
        "id": "HvDqXq5zHNrC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# property extensions\n",
        "from spacy.tokens import Token\n",
        "\n",
        "# Define getter function\n",
        "def get_is_color(token):\n",
        "  colors = [\"red\",\"yello\",\"blue\"]\n",
        "  return token.text in colors\n",
        "\n",
        "# set extension on the Token with getter\n",
        "Token.set_extension(\"is_colorful\", getter=get_is_color)\n",
        "\n",
        "doc = nlp(\"The sky is blue\")\n",
        "print(doc[3]._.is_colorful, \"-\", doc[3].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGi7VplmHqQj",
        "outputId": "d6f12a9b-f0a0-4fc4-b049-2da0c4336599"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True - blue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Span extentions should almost always use a getter\n",
        "def get_has_color(span):\n",
        "    colors = [\"red\", \"yellow\", \"blue\"]\n",
        "    return any(token.text in colors for token in span)\n",
        "\n",
        "# Set extension on the Span with getter\n",
        "Span.set_extension(\"has_color\", getter=get_has_color, force=True)\n",
        "\n",
        "doc = nlp(\"The sky is blue.\")\n",
        "print(doc[1:4]._.has_color, \"-\", doc[1:4].text)\n",
        "print(doc[0:2]._.has_color, \"-\", doc[0:2].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMAPxiojJfTj",
        "outputId": "bfb6c9cd-1400-4993-ccc3-0999c8d78eb7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True - sky is blue\n",
            "False - The sky\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method extension\n",
        "# Method extensions make the extension attribute a callable method.\n",
        "# You can then pass one or more arguments to it, and compute attribute values dynamically – for example, based on a certain argument or setting.\n",
        "# In this example, the method function checks whether the doc contains a token with a given text. The first argument of the method is always the object itself – in this case, the doc. It's passed in automatically when the method is called. All other function arguments will be arguments on the method extension. In this case, token_text.\n",
        "# Here, the custom ._.has_token method returns True for the word \"blue\" and False for the word \"cloud\".\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# Define method with arguments\n",
        "def has_token(doc, token_text):\n",
        "  in_doc = token_text in [token.text for token in doc]\n",
        "  return in_doc\n",
        "\n",
        "# Set extension on the Doc with method\n",
        "Doc.set_extension(\"has_token\", method = has_token)\n",
        "\n",
        "doc = nlp(\"The sky is blue\")\n",
        "print(doc._.has_token(\"blue\"), \"-blue\")\n",
        "print(doc._.has_token(\"cloud\"), \"- cloud\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNmTosxmKYzq",
        "outputId": "275c57d2-3d0e-4011-e3ce-35aeed090b63"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True -blue\n",
            "False - cloud\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entities and extensions\n",
        "\"\"\"\n",
        "In this exercise, you’ll combine custom extension attributes with the statistical predictions and create an attribute getter that returns a Wikipedia search URL if the span is a person, organization, or location.\n",
        "\n",
        "Complete the get_wikipedia_url getter so it only returns the URL if the span’s label is in the list of labels.\n",
        "Set the Span extension \"wikipedia_url\" using the getter get_wikipedia_url.\n",
        "Iterate over the entities in the doc and output their Wikipedia URL.\n",
        "\"\"\"\n",
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "\n",
        "def get_wikipedia_url(span):\n",
        "    # Get a Wikipedia URL if the span has one of the labels\n",
        "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
        "        entity_text = span.text.replace(\" \", \"_\")\n",
        "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
        "\n",
        "\n",
        "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
        "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url, force=True)\n",
        "\n",
        "doc = nlp(\n",
        "    \"In over fifty years from his very first recordings right through to his \"\n",
        "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
        ")\n",
        "for ent in doc.ents:\n",
        "    # Print the text and Wikipedia URL of the entity\n",
        "    print(ent.text, ent._.wikipedia_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_p7CNq3NBgf",
        "outputId": "90620c4f-d66e-4e64-c578-e3a484121574"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "over fifty years None\n",
            "first None\n",
            "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling and performance"
      ],
      "metadata": {
        "id": "hUDPNdyOQR7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# passing in context\n",
        "data = [\n",
        "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
        "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
        "]\n",
        "\n",
        "for doc, context in nlp.pipe(data, as_tuples=True):\n",
        "    print(doc.text, context[\"page_number\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0xljIvlQRa7",
        "outputId": "02b8ffad-1091-4bac-a14f-37de83ce9cf7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a text 15\n",
            "And another text 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Doc\n",
        "\n",
        "Doc.set_extension(\"id\", default=None)\n",
        "Doc.set_extension(\"page_number\", default=None)\n",
        "\n",
        "data = [\n",
        "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
        "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
        "]\n",
        "\n",
        "for doc, context in nlp.pipe(data, as_tuples=True):\n",
        "    doc._.id = context[\"id\"]\n",
        "    doc._.page_number = context[\"page_number\"]"
      ],
      "metadata": {
        "id": "-YSaZFUFQzsX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use nlp.make_doc to turn a text into a Doc object\n",
        "doc = nlp.make_doc(\"Hello world\")"
      ],
      "metadata": {
        "id": "gW0DVP7uRAOR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}